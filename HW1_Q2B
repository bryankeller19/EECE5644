import matplotlib.pyplot as plt # For general plotting
import numpy as np
from scipy.stats import multivariate_normal # MVN not univariate
from sklearn.metrics import confusion_matrix
np.set_printoptions(suppress=True)

# Set seed to generate reproducible "pseudo-randomness" (handles scipy's "randomness" too)
np.random.seed(7)

plt.rc('font', size=22)          # controls default text sizes
plt.rc('axes', titlesize=18)     # fontsize of the axes title
plt.rc('axes', labelsize=18)    # fontsize of the x and y labels
plt.rc('xtick', labelsize=14)    # fontsize of the tick labels
plt.rc('ytick', labelsize=14)    # fontsize of the tick labels
plt.rc('legend', fontsize=18)    # legend fontsize
plt.rc('figure', titlesize=22)  # fontsize of the figure title

N = 10000 #number of samples

# Mean and covariance of data pdfs conditioned on labels
mu = np.array([[-4, -2],
               [-2, 0],
               [0, 2],
               [2, 4]])  # Gaussian distributions means
Sigma = np.array([[[1, 0],
                   [0, 1]],
                  [[2, 0],
                   [0, 2]],
                  [[3, 0],
                   [0, 3]],
                  [[4, 0],
                   [0, 4]]])   # Gaussian distributions covariance matrices
# Determine dimensionality from mixture PDF parameters
n = mu.shape[1]

# Class priors
priors = np.array([0.2,0.25,0.25,0.3])  
C = len(priors)

# Decide randomly which samples will come from each component
u = np.random.rand(N) #10k random samples
thresholds = np.cumsum(priors)
thresholds = np.insert(thresholds, 0, 0) # For intervals of classes (0 0 0.15 0.5 1)

# Output samples and labels
X = np.zeros([N, n]) 
y = np.zeros(N)
labels = np.zeros(N) # KEEP TRACK OF THIS - 10000
 
# Plot for original data and their true labels
fig = plt.figure(figsize=(10, 10))
marker_shapes = 'dx^.'
marker_colors = 'rbgc' 

L = np.array(range(1, C+1))
for l in L:
    # Get randomly sampled indices for this component
    indices = np.argwhere((thresholds[l-1] <= u) & (u <= thresholds[l]))[:, 0]
    # No. of samples in this component
    Nl = len(indices)  
    labels[indices] = l * np.ones(Nl)
    X[indices, :] =  multivariate_normal.rvs(mu[l-1], Sigma[l-1], Nl) #normal distribution
    plt.plot(X[labels==l, 0], X[labels==l, 1], marker_shapes[l-1] + marker_colors[l-1], label="True Class {}".format(l))
    
# Plot the original data and their true labels
plt.legend()
plt.xlabel(r"$x_1$")
plt.ylabel(r"$x_2$")
plt.title("Generated Original Data Samples")
plt.tight_layout()
plt.show()

L = np.array(range(C))
Nl = np.array([sum(labels == l) for l in L]) #gets number of samples from each class
print("Number of samples from Class 1: {:d}, Class 2: {:d}, Class 3: {:d}, Class 4: {:d}".format(Nl[0], Nl[1], Nl[2], Nl[3]))

# MAP classifier (is a special case of ERM corresponding to 0-1 loss)
# 0-1 loss values yield MAP decision rule
Lambda = np.array([[0,1,2,3],
                   [1,0,1,2],
                   [2,1,0,1],
                   [3,2,1,0]])
class_cond_likelihoods = np.array([multivariate_normal.pdf(X, mu[l], Sigma[l]) for l in L])
class_priors = np.diag(priors)
class_posteriors = class_priors.dot(class_cond_likelihoods)

# We want to create the risk matrix of size 4 x N 
cond_risk = Lambda.dot(class_posteriors)

# Get the decision for each column in risk_mat
decisions = np.argmin(cond_risk, axis=0)+ np.ones(N) 

# Confusion matrix
print("Confusion Matrix (rows: Predicted class, columns: True class):")
conf_mat = confusion_matrix(decisions, labels)
print(conf_mat)
correct_class_samples = np.sum(np.diag(conf_mat))
print("Total Mumber of Misclassified Samples: {:d}".format(N - correct_class_samples))
prob_error = 1 - (correct_class_samples / N)
print("Empirically Estimated Probability of Error: {:.4f}".format(prob_error))


# Plot for decisions vs true labels
fig = plt.figure(figsize=(10, 10))

for r in L: # Each decision option
    for c in L: # Each class label
        ind_rc = np.argwhere((decisions==r) & (labels==c))

        # Decision = Marker Shape; True Labels = Marker Color
        marker = marker_shapes[r-1] + marker_colors[c-1]
        if r == c:
            plt.plot(X[ind_rc, 0], X[ind_rc, 1], marker_shapes[r-1] + "g", label="Correctly Predicted as Class {}".format(c+1))
        elif c == 3:
            plt.plot(X[ind_rc, 0], X[ind_rc, 1], marker_shapes[r-1] + "r", markersize=8, label="Incorrectly Predicted as Class {}".format(r+1))
        else:
            plt.plot(X[ind_rc, 0], X[ind_rc, 1], marker_shapes[r-1] + "r", markersize=8)

plt.legend()
plt.xlabel(r"$x_1$")
plt.ylabel(r"$x_2$")
plt.title("Classification Decisions: Marker Shape/Predictions, Color/True Labels")
plt.tight_layout()
plt.show()
